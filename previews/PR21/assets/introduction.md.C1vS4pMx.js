import{_ as s,c as a,o as e,a7 as n}from"./chunks/framework.BnBE8hm_.js";const m=JSON.parse('{"title":"Introduction","description":"","frontmatter":{},"headers":[],"relativePath":"introduction.md","filePath":"introduction.md","lastUpdated":null}'),t={name:"introduction.md"},i=n(`<h1 id="Introduction" tabindex="-1">Introduction <a class="header-anchor" href="#Introduction" aria-label="Permalink to &quot;Introduction {#Introduction}&quot;">​</a></h1><p>Welcome to AIHelpMe.jl, your go-to for getting answers to your Julia coding questions.</p><p>AIhelpMe is a simple wrapper around RAG functionality in PromptingTools.</p><p>It provides three extras:</p><ul><li><p>(hopefully), a simpler interface to handle RAG configurations (there are thousands of possible configurations)</p></li><li><p>pre-computed embeddings for key “knowledge” in the Julia ecosystem (we refer to them as “knowledge packs”)</p></li><li><p>ability to quickly incorporate any additional knowledge (eg, your currently loaded packages) into the &quot;assistant&quot;</p></li></ul><div class="caution custom-block github-alert"><p class="custom-block-title">This is only a prototype! We have not tuned it yet, so your mileage may vary! Always check your results from LLMs!</p><p></p></div><h2 id="What-is-RAG?" tabindex="-1">What is RAG? <a class="header-anchor" href="#What-is-RAG?" aria-label="Permalink to &quot;What is RAG? {#What-is-RAG?}&quot;">​</a></h2><p>RAG, short for Retrieval-Augmented Generation, is a way to reduce hallucinations of your model and improve its response by directly providing the relevant source knowledge into the prompt.</p><p>See more details <a href="https://aws.amazon.com/what-is/retrieval-augmented-generation" target="_blank" rel="noreferrer">here</a>.</p><h2 id="What-is-a-knowledge-pack?" tabindex="-1">What is a knowledge pack? <a class="header-anchor" href="#What-is-a-knowledge-pack?" aria-label="Permalink to &quot;What is a knowledge pack? {#What-is-a-knowledge-pack?}&quot;">​</a></h2><p>A knowledge pack is a collection of pre-computed embeddings for a specific set of documents. The documents are typically related to a specific topic or area of interest or package ecosystem (eg, Makie.jl documentation site with all its packages). The embeddings are computed using a specific embedding model and are used to improve the quality of the generated responses by providing the relevant source knowledge directly to the prompt.</p><p>We currently provide 3 knowledge packs:</p><ul><li><p><code>:julia</code> - Julia documentation, standard library docstrings and a few extras</p></li><li><p><code>:tidier</code> - Tidier.jl organization documentation</p></li><li><p><code>:makie</code> - Makie.jl organization documentation</p></li></ul><p>You can load all of them with <code>AIHelpMe.load_index!([:julia, :tidier, :makie])</code>.</p><p>It is EXTREMELY IMPORTANT to use the SAME embedding model in <code>aihelp()</code> queries as the one used to build the knowledge pack. That is why you have to be careful changing the RAG pipeline configuration - always use the dedicated utility <code>update_pipeline!()</code>.</p><h2 id="Installation" tabindex="-1">Installation <a class="header-anchor" href="#Installation" aria-label="Permalink to &quot;Installation {#Installation}&quot;">​</a></h2><p>To install AIHelpMe, use the Julia package manager and the package name:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Pkg</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Pkg</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">add</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;AIHelpMe&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p><strong>Prerequisites:</strong></p><ul><li><p>Julia (version 1.10 or later).</p></li><li><p>Internet connection for API access.</p></li><li><p>OpenAI API keys with available credits. See <a href="/svilupp.github.io/AIHelpMe.jl/previews/PR21/introduction#how-to-obtain-api-keys">How to Obtain API Keys</a>.</p></li><li><p>For optimal performance, get also Cohere API key (free for community use) and Tavily API key (free for community use).</p></li></ul><p>All setup should take less than 5 minutes!</p><h2 id="Quick-Start-Guide" tabindex="-1">Quick Start Guide <a class="header-anchor" href="#Quick-Start-Guide" aria-label="Permalink to &quot;Quick Start Guide {#Quick-Start-Guide}&quot;">​</a></h2><ol><li><strong>Basic Usage</strong>:</li></ol><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AIHelpMe</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">aihelp</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;How do I implement quicksort in Julia?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span>[ Info: Done generating response. Total cost: \\$0.015</span></span>
<span class="line"><span>AIMessage(&quot;To implement quicksort in Julia, you can use the \`sort\` function with the \`alg=QuickSort\` argument.&quot;)</span></span></code></pre></div><p>Note: As a default, we load only the Julia documentation and docstrings for standard libraries. The default model used is GPT-4 Turbo. You can pretty-print the answer using <code>pprint</code> if you return the full RAGResult (<code>return_all=true</code>):</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AIHelpMe</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pprint</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">result </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> aihelp</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;How do I implement quicksort in Julia?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, return_all</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">pprint</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(result)</span></span></code></pre></div><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span>--------------------</span></span>
<span class="line"><span>QUESTION(s)</span></span>
<span class="line"><span>--------------------</span></span>
<span class="line"><span>- How do I implement quicksort in Julia?</span></span>
<span class="line"><span></span></span>
<span class="line"><span>--------------------</span></span>
<span class="line"><span>ANSWER</span></span>
<span class="line"><span>--------------------</span></span>
<span class="line"><span>To implement quicksort in Julia, you can use the [5,1.0]\`sort\`[1,1.0] function with the [1,1.0]\`alg=QuickSort\`[1,1.0] argument.[2,1.0]</span></span>
<span class="line"><span></span></span>
<span class="line"><span>--------------------</span></span>
<span class="line"><span>SOURCES</span></span>
<span class="line"><span>--------------------</span></span>
<span class="line"><span>1. https://docs.julialang.org/en/v1.10.2/base/sort/index.html::Sorting and Related Functions/Sorting Functions</span></span>
<span class="line"><span>2. https://docs.julialang.org/en/v1.10.2/base/sort/index.html::Sorting and Related Functions/Sorting Functions</span></span>
<span class="line"><span>3. https://docs.julialang.org/en/v1.10.2/base/sort/index.html::Sorting and Related Functions/Sorting Algorithms</span></span>
<span class="line"><span>4. SortingAlgorithms::/README.md::0::SortingAlgorithms</span></span>
<span class="line"><span>5. AIHelpMe::/README.md::0::AIHelpMe</span></span></code></pre></div><p>Note: You can see the model cheated because it can see this very documentation...</p><ol><li><strong><code>aihelp</code> Macro</strong>:</li></ol><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">aihelp</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;how to implement quicksort in Julia?&quot;</span></span></code></pre></div><ol><li><strong>Follow-up Questions</strong>:</li></ol><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">aihelp!</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Can you elaborate on the \`sort\` function?&quot;</span></span></code></pre></div><p>Note: The <code>!</code> is required for follow-up questions. <code>aihelp!</code> does not add new context/more information - to do that, you need to ask a new question.</p><ol><li><p><strong>Pick faster models</strong>: Eg, for simple questions, GPT 3.5 might be enough, so use the alias &quot;gpt3t&quot;: <code>julia aihelp&quot;Elaborate on the </code>sort<code> function and quicksort algorithm&quot;gpt3t</code><code>plaintext [ Info: Done generating response. Total cost: \\$0.002 --&gt; AIMessage(&quot;The </code>sort<code> function in programming languages, including Julia.... continues for a while!</code></p></li><li><p><strong>Debugging</strong>: How did you come up with that answer? Check the &quot;context&quot; provided to the AI model (ie, the documentation snippets that were used to generate the answer): \`\`\`julia using AIHelpMe: pprint, last_result pprint(last_result())</p></li></ol><h1 id="output-pretty-printed-question-context-answer-with-color-highlights" tabindex="-1">Output: Pretty-printed Question + Context + Answer with color highlights <a class="header-anchor" href="#output-pretty-printed-question-context-answer-with-color-highlights" aria-label="Permalink to &quot;Output: Pretty-printed Question + Context + Answer with color highlights&quot;">​</a></h1><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span>The color highlights show you which words were NOT supported by the provided context (magenta = completely new, blue = partially new).   It&amp;#39;s an intuitive way to see which function names or variables are made up versus which ones were in the context. </span></span>
<span class="line"><span>You can change the kwargs of \`pprint\` to hide the annotations or potentially even show the underlying context (snippets from the documentation):</span></span>
<span class="line"><span>\`julia  pprint(last_result(); add_context = true, add_scores = false)\`</span></span>
<span class="line"><span></span></span>
<span class="line"><span>&gt; </span></span>
<span class="line"><span>&gt; [!TIP] Your results will significantly improve if you enable re-ranking of the context to be provided to the model (eg, \`aihelp(..., rerank=true)\`) or change pipeline to \`update_pipeline!(:silver)\`. It requires setting up Cohere API key but it&amp;#39;s free for community use.</span></span>
<span class="line"><span>&gt; </span></span>
<span class="line"><span>&gt; </span></span>
<span class="line"><span>&gt; [!TIP] Do you want to safely execute the generated code? Use \`AICode\` from \`PromptingTools.Experimental.AgentToolsAI.\`. It can executed the code in a scratch module and catch errors if they happen (eg, apply directly to \`AIMessage\` response like \`AICode(msg)\`).</span></span>
<span class="line"><span>&gt; </span></span>
<span class="line"><span></span></span>
<span class="line"><span>Noticed some weird answers? Please let us know! See [Help Us Improve and Debug](/advanced#Help-Us-Improve-and-Debug).</span></span>
<span class="line"><span></span></span>
<span class="line"><span>If you want to use locally-hosted models, see the [Using Ollama Models](/advanced#Using-Ollama-Models) section.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>If you want to customize your setup, see \`AIHelpMe.PREFERENCES\`.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>## How to Obtain API Keys {#How-to-Obtain-API-Keys}</span></span>
<span class="line"><span></span></span>
<span class="line"><span>### OpenAI API Key: {#OpenAI-API-Key:}</span></span>
<span class="line"><span>1. Visit [OpenAI&amp;#39;s API portal](https://openai.com/api/).</span></span>
<span class="line"><span></span></span>
<span class="line"><span>1. Sign up and generate an API Key.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>1. Charge some credits ($5 minimum but that will last you for a lost time).</span></span>
<span class="line"><span></span></span>
<span class="line"><span>1. Set it as an environment variable or a local preference in PromptingTools.jl. See the [instructions](https://siml.earth/PromptingTools.jl/dev/frequently_asked_questions#Configuring-the-Environment-Variable-for-API-Key).</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>### Cohere API Key: {#Cohere-API-Key:}</span></span>
<span class="line"><span>1. Sign up at [Cohere&amp;#39;s registration page](https://dashboard.cohere.com/welcome/register).</span></span>
<span class="line"><span></span></span>
<span class="line"><span>1. After registering, visit the [API keys section](https://dashboard.cohere.com/api-keys) to obtain a free, rate-limited Trial key.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>1. Set it as an environment variable or a local preference in PromptingTools.jl. See the [instructions](https://siml.earth/PromptingTools.jl/dev/frequently_asked_questions#Configuring-the-Environment-Variable-for-API-Key).</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>### Tavily API Key: {#Tavily-API-Key:}</span></span>
<span class="line"><span>1. Sign up at [Tavily](https://app.tavily.com/sign-in).</span></span>
<span class="line"><span></span></span>
<span class="line"><span>1. After registering, generate an API key on the [Overview page](https://app.tavily.com/home). You can get a free, rate-limited Trial key.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>1. Set it as an environment variable or a local preference in PromptingTools.jl. See the [instructions](https://siml.earth/PromptingTools.jl/dev/frequently_asked_questions#Configuring-the-Environment-Variable-for-API-Key).</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>## Usage {#Usage}</span></span>
<span class="line"><span></span></span>
<span class="line"><span>**Formulating Questions**:</span></span>
<span class="line"><span>- Be clear and specific for the best results. Do mention the programming language (eg, Julia) and the topic (eg, &amp;quot;quicksort&amp;quot;) when it&amp;#39;s not obvious from the context.</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>**Example Queries**:</span></span>
<span class="line"><span>- Simple question: \`aihelp&quot;What is a DataFrame in Julia?&quot;\`</span></span>
<span class="line"><span></span></span>
<span class="line"><span>- Using a model: \`aihelp&quot;best practices for error handling in Julia&quot;gpt4t\`</span></span>
<span class="line"><span></span></span>
<span class="line"><span>- Follow-up: \`aihelp!&quot;Could you provide an example?&quot;\`</span></span>
<span class="line"><span></span></span>
<span class="line"><span>- Debug errors (use \`err\` REPL variable):</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>\`\`\`julia</span></span>
<span class="line"><span>## define mock function to trigger method error</span></span>
<span class="line"><span>f(x::Int) = x^2</span></span>
<span class="line"><span>f(Int8(2))</span></span>
<span class="line"><span># we get: ERROR: MethodError: no method matching f(::Int8)</span></span>
<span class="line"><span></span></span>
<span class="line"><span># Help is here:</span></span>
<span class="line"><span>aihelp&quot;What does this error mean? \\$err&quot; # Note the $err to interpolate the stacktrace</span></span></code></pre></div><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span>[ Info: Done generating response. Total cost: \\$0.003</span></span>
<span class="line"><span></span></span>
<span class="line"><span>AIMessage(&quot;The error message &quot;MethodError: no method matching f(::Int8)&quot; means that there is no method defined for function \`f\` that accepts an argument of type \`Int8\`. The error message also provides the closest candidate methods that were found, which are \`f(::Any, !Matched::Any)\` and \`f(!Matched::Int64)\` in the specified file \`embed_all.jl\` at lines 45 and 61, respectively.&quot;)</span></span></code></pre></div><h2 id="How-it-works" tabindex="-1">How it works <a class="header-anchor" href="#How-it-works" aria-label="Permalink to &quot;How it works {#How-it-works}&quot;">​</a></h2><p>AIHelpMe leverages <a href="https://github.com/svilupp/PromptingTools.jl" target="_blank" rel="noreferrer">PromptingTools.jl</a> to communicate with the AI models.</p><p>We apply a Retrieval Augment Generation (RAG) pattern, ie,</p><ul><li><p>we pre-process all available documentation (and &quot;embed it&quot; to convert text snippets into numbers)</p></li><li><p>when a question is asked, we look up the most relevant documentation snippets</p></li><li><p>we feed the question and the documentation snippets to the AI model</p></li><li><p>the AI model generates the answer</p></li><li><p>(sometimes) we run a web search to find additional relevant information and ask the model to refine the answer</p></li></ul><p>This ensures that the answers are not only based on general AI knowledge but are also specifically tailored to Julia&#39;s ecosystem and best practices.</p><p>Visit an introduction to RAG tools in <a href="https://svilupp.github.io/PromptingTools.jl/dev/extra_tools/rag_tools_intro" target="_blank" rel="noreferrer">PromptingTools.jl</a>.</p><h2 id="Future-Directions" tabindex="-1">Future Directions <a class="header-anchor" href="#Future-Directions" aria-label="Permalink to &quot;Future Directions {#Future-Directions}&quot;">​</a></h2><p>AIHelpMe is continuously evolving. Future updates may include:</p><ul><li><p>Tools to trace the provenance of answers (ie, where did the answer come from?).</p></li><li><p>Creation of a gold standard Q&amp;A dataset for evaluation.</p></li><li><p>Refinement of the RAG ingestion pipeline for optimized chunk processing and deduplication.</p></li><li><p>Introduction of context filtering to focus on specific modules.</p></li><li><p>Transition to a more sophisticated multi-turn conversation design.</p></li><li><p>Enhancement of the marginal information provided by the RAG context.</p></li><li><p>Expansion of content sources beyond docstrings, potentially including documentation sites and community resources like Discourse or Slack posts.</p></li></ul><p>Please note that this is merely a pre-release to gauge the interest in this project.</p>`,48),p=[i];function o(l,r,c,h,d,u){return e(),a("div",null,p)}const k=s(t,[["render",o]]);export{m as __pageData,k as default};
