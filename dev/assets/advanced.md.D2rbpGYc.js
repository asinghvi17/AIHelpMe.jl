import{_ as e,c as a,o as s,a7 as i}from"./chunks/framework.BMhi3Yu-.js";const u=JSON.parse('{"title":"Advanced","description":"","frontmatter":{},"headers":[],"relativePath":"advanced.md","filePath":"advanced.md","lastUpdated":null}'),n={name:"advanced.md"},t=i(`<h1 id="Advanced" tabindex="-1">Advanced <a class="header-anchor" href="#Advanced" aria-label="Permalink to &quot;Advanced {#Advanced}&quot;">​</a></h1><p>To be updated...</p><h2 id="Using-Ollama-models" tabindex="-1">Using Ollama models <a class="header-anchor" href="#Using-Ollama-models" aria-label="Permalink to &quot;Using Ollama models {#Using-Ollama-models}&quot;">​</a></h2><p>AIHelpMe can use Ollama models, but the knowledge packs are available for only one embedding model: &quot;nomic-embed-text&quot;!</p><p>You must set <code>model_embedding=&quot;nomic-embed-text&quot;</code> and <code>truncate_dimension=0</code> (maximum dimension available) for everything to work correctly!</p><p>Example:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PromptingTools</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> register_model!, OllamaSchema</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AIHelpMe</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> update_pipeline!, load_index!</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># register a chat model in Ollama schema</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">register_model!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(; name</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;mistral:7b-instruct-v0.2-q4_K_M&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,schema</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">OllamaSchema</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">())</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># you can use whichever chat model you like!</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">update_pipeline!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:bronze</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; model_chat </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;mistral:7b-instruct-v0.2-q4_K_M&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,model_embedding</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;nomic-embed-text&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, truncate_dimension</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># You must download the corresponding knowledge packs via \`load_index!\` (because you changed the embedding model)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">load_index!</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">:julia</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># or whichever other packs you want!</span></span></code></pre></div><p>Let&#39;s ask a question:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">aihelp</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;How to create a named tuple?&quot;</span></span></code></pre></div><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span>[ Info: Done with RAG. Total cost: \\$0.0</span></span>
<span class="line"><span>PromptingTools.AIMessage(&quot;In Julia, you can create a named tuple by enclosing key-value pairs in parentheses with the keys as symbols preceded by a colon, separated by commas. For example:</span></span>
<span class="line"><span>...continues</span></span></code></pre></div><h2 id="Extending-the-Knowledge-Base" tabindex="-1">Extending the Knowledge Base <a class="header-anchor" href="#Extending-the-Knowledge-Base" aria-label="Permalink to &quot;Extending the Knowledge Base {#Extending-the-Knowledge-Base}&quot;">​</a></h2><p>The package by default ships with pre-processed embeddings for all Julia standard libraries, DataFrames and PromptingTools. Thanks to the amazing Julia Artifacts system, these embeddings are downloaded/cached/loaded every time the package starts.</p><p>Note: The below functions are not yet exported. Prefix them with <code>AIHelpMe.</code> to use them.</p><h3 id="Building-and-Updating-Indexes" tabindex="-1">Building and Updating Indexes <a class="header-anchor" href="#Building-and-Updating-Indexes" aria-label="Permalink to &quot;Building and Updating Indexes {#Building-and-Updating-Indexes}&quot;">​</a></h3><p>AIHelpMe allows users to enhance its capabilities by embedding documentation from any loaded Julia module. Utilize <code>new_index = build_index(module)</code> to create an index for a specific module (or a vector of modules).</p><p>To update an existing index, including newly imported packages, use <code>new index = update_index(module)</code> or simply <code>update_index()</code> to include all unrecognized modules. We will add and embed only the new documentation to avoid unnecessary duplication and cost.</p><h3 id="Managing-Indexes" tabindex="-1">Managing Indexes <a class="header-anchor" href="#Managing-Indexes" aria-label="Permalink to &quot;Managing Indexes {#Managing-Indexes}&quot;">​</a></h3><p>Once an index is built or updated, you can choose to serialize it for later use or set it as the primary index.</p><p>To use your newly created index as the main source for queries, execute <code>load_index!(new_index)</code>. Alternatively, load a pre-existing index from a file using <code>load_index!(file_path)</code>.</p><p>The main index for queries is held in the global variable <code>AIHelpMe.MAIN_INDEX[]</code>.</p>`,20),l=[t];function d(p,o,h,r,c,k){return s(),a("div",null,l)}const m=e(n,[["render",d]]);export{u as __pageData,m as default};
