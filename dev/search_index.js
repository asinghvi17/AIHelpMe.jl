var documenterSearchIndex = {"docs":
[{"location":"reference/","page":"Reference","title":"Reference","text":"CurrentModule = AIHelpMe","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [AIHelpMe]","category":"page"},{"location":"reference/#AIHelpMe.ALLOWED_PACKS","page":"Reference","title":"AIHelpMe.ALLOWED_PACKS","text":"ALLOWED PACKS\n\nCurrently available packs are:\n\n:julia - Julia documentation, standard library docstrings and a few extras (for Julia v1.10)\n:tidier - Tidier.jl organization documentation (as of 7th April 2024)\n:makie - Makie.jl organization documentation and a few extras (as of 30th March 2024)\n\n\n\n\n\n","category":"constant"},{"location":"reference/#AIHelpMe.RAG_CONFIGURATIONS","page":"Reference","title":"AIHelpMe.RAG_CONFIGURATIONS","text":"RAG_CONFIGURATIONS\n\nA dictionary of RAG configurations, keyed by a unique symbol (eg, bronze). Each entry contains a dictionary with keys :config and :kwargs, where :config is the RAG configuration object (AbstractRAGConfig) and :kwargs the NamedTuple of corresponding kwargs.\n\nAvailable Options:\n\n:bronze: A simple configuration for a bronze pipeline, using truncated binary embeddings (dimensionality: 1024) and no re-ranking or refinement.\n:silver: A simple configuration for a bronze pipeline, using truncated binary embeddings (dimensionality: 1024) but also enables re-ranking and refinement with a web-search.\n:gold: A more complex configuration, similar to :simpler, but using a standard embeddings (dimensionality: 3072, type: Float32). It also leverages re-ranking and refinement with a web-search.\n\n\n\n\n\n","category":"constant"},{"location":"reference/#AIHelpMe.aihelp-Tuple{PromptingTools.Experimental.RAGTools.AbstractRAGConfig, PromptingTools.Experimental.RAGTools.AbstractChunkIndex, AbstractString}","page":"Reference","title":"AIHelpMe.aihelp","text":"aihelp([cfg::RT.AbstractRAGConfig, index::RT.AbstractChunkIndex,]\n    question::AbstractString;\n    verbose::Integer = 1,\n    model = MODEL_CHAT,\n    return_all::Bool = false)\n\nGenerates a response for a given question using a Retrieval-Augmented Generation (RAG) approach over Julia documentation (or any other knowledge pack). \n\nIf you return RAGResult (return_all=true), you can use AIHelpMe.pprint to pretty-print the result and see the sources/\"support\" scores for each chunk of the answer.\n\nThe answer will depend on the knowledge packs loaded, see ?load_index!.\n\nYou can also use add docstrings from any package you have loaded (or all of them), see ?update_index and make sure to provide your new updated index explicitly!\n\nArguments\n\ncfg::AbstractRAGConfig: The RAG configuration.\nindex::AbstractChunkIndex: The chunk index (contains chunked and embedded documentation).\nquestion::AbstractString: The question to be answered.\nmodel::String: A chat/generation model used for generating the final response, default is MODEL_CHAT.\nreturn_all::Bool: If true, returns a RAGResult (provides details of the pipeline + allows pretty-printing with pprint(result)).\nsearch::Union{Nothing, Bool}: If true, uses TavilySearchRefiner to add web search results to the context. See ?PromptingTools.Experimental.RAGTools.TavilySearchRefiner for details.\nrerank::Union{Nothing, Bool}: If true, uses CohereReranker to rerank the chunks. See ?PromptingTools.Experimental.RAGTools.CohereReranker for details.\n\nReturns\n\nIf return_all is false, returns the generated message (msg).\nIf return_all is true, returns a RAGResult (provides details of the pipeline + allows pretty-printing with pprint(result))\n\nNotes\n\nFunction always saves the last context in global LAST_RESULT for inspection of sources/context regardless of return_all value.\n\nExamples\n\nUsing aihelp to get a response for a question:\n\nusing AIHelpMe: build_index\n\nindex = build_index(...)  # create an index that contains Makie.jl documentation (or any loaded package that you have)\n\nquestion = \"How to make a barplot in Makie.jl?\"\nmsg = aihelp(index, question)\n\nIf you want a pretty-printed answer with highlighted sources, you can use the return_all argument and pprint utility:\n\nusing AIHelpMe: pprint\n\nresult = aihelp(index, question; return_all = true)\npprint(result)\n\nIf you loaded a knowledge pack, you do not have to provide the index.\n\n# Load Makie knowledge pack\nAIHelpMe.load_index!(:makie)\n\nquestion = \"How to make a barplot in Makie.jl?\"\nmsg = aihelp(question)\n\nIf you know it's a hard question, you can use the search and rerank arguments to add web search results to the context and rerank the chunks.\n\nusing AIHelpMe: pprint\n\nquestion = \"How to make a barplot in Makie.jl?\"\nresult = aihelp(question; search = true, rerank = true, return_all = true)\npprint(result) # nicer display with sources for each chunk/sentences (look for square brackets)\n\n\n\n\n\n","category":"method"},{"location":"reference/#AIHelpMe.docdata_to_source-Tuple{AbstractDict}","page":"Reference","title":"AIHelpMe.docdata_to_source","text":"docdata_to_source(data::AbstractDict)\n\nCreates a source path from a given DocStr record\n\n\n\n\n\n","category":"method"},{"location":"reference/#AIHelpMe.docextract","page":"Reference","title":"AIHelpMe.docextract","text":"docextract(d::MultiDoc, sep::AbstractString = \"\n\n\")\n\nExtracts the documentation from a MultiDoc record (separates the individual docs within DocStr with sep)\n\n\n\n\n\n","category":"function"},{"location":"reference/#AIHelpMe.docextract-2","page":"Reference","title":"AIHelpMe.docextract","text":"docextract(modules::Vector{Module} = Base.Docs.modules)\n\nExtracts the documentation from a vector of modules.\n\n\n\n\n\n","category":"function"},{"location":"reference/#AIHelpMe.docextract-3","page":"Reference","title":"AIHelpMe.docextract","text":"docextract(d::DocStr, sep::AbstractString = \"\n\n\")\n\nExtracts the documentation from a DocStr record. Separates the individual docs within DocStr with sep.\n\n\n\n\n\n","category":"function"},{"location":"reference/#AIHelpMe.docextract-Tuple{Module}","page":"Reference","title":"AIHelpMe.docextract","text":"docextract(mod::Module)\n\nExtracts the documentation from a given (loaded) module.\n\n\n\n\n\n","category":"method"},{"location":"reference/#AIHelpMe.find_new_chunks-Tuple{AbstractVector{<:AbstractString}, AbstractVector{<:AbstractString}}","page":"Reference","title":"AIHelpMe.find_new_chunks","text":"find_new_chunks(old_chunks::AbstractVector{<:AbstractString},\n    new_chunks::AbstractVector{<:AbstractString})\n\nIdentifies the new chunks in new_chunks that are not present in old_chunks.\n\nReturns a mask of chunks that are new (not present in old_chunks).\n\nUses SHA256 hashes to dedupe the strings quickly and effectively.\n\n\n\n\n\n","category":"method"},{"location":"reference/#AIHelpMe.get_config_key-Tuple{PromptingTools.Experimental.RAGTools.AbstractRAGConfig, NamedTuple}","page":"Reference","title":"AIHelpMe.get_config_key","text":"Returns the configuration key for the given cfg and kwargs to use the relevant artifacts.\n\n\n\n\n\n","category":"method"},{"location":"reference/#AIHelpMe.last_result-Tuple{}","page":"Reference","title":"AIHelpMe.last_result","text":"last_result()\n\nReturns the RAGResult from the last aihelp call.  It can be useful to see the sources/references used by the AI model to generate the response.\n\nIf you're using aihelp() make sure to set return_all = true to return the RAGResult.\n\n\n\n\n\n","category":"method"},{"location":"reference/#AIHelpMe.load_index!-Tuple{AbstractString}","page":"Reference","title":"AIHelpMe.load_index!","text":"load_index!(file_path::AbstractString;\n    verbose::Bool = true, kwargs...)\n\nLoads the serialized index in file_path into the global variable MAIN_INDEX.\n\nSupports .jls (serialized Julia object) and .hdf5 (HDF5.jl) files.\n\n\n\n\n\n","category":"method"},{"location":"reference/#AIHelpMe.load_index!-Tuple{PromptingTools.Experimental.RAGTools.AbstractChunkIndex}","page":"Reference","title":"AIHelpMe.load_index!","text":"load_index!(index::RT.AbstractChunkIndex;\n    verbose::Bool = 1, kwargs...)\n\nLoads the provided index into the global variable MAIN_INDEX.\n\nIf you don't have an index yet, use build_index to build one from your currently loaded packages (see ?build_index)\n\nExample\n\n# build an index from some modules, keep empty to embed all loaded modules (eg, `build_index()`) \nindex = AIH.build_index([DataFramesMeta, DataFrames, CSV])\nAIH.load_index!(index)\n\n\n\n\n\n","category":"method"},{"location":"reference/#AIHelpMe.load_index!-Tuple{Vector{Symbol}}","page":"Reference","title":"AIHelpMe.load_index!","text":"load_index!(packs::Vector{Symbol}; verbose::Bool = true, kwargs...)\nload_index!(pack::Symbol; verbose::Bool = true, kwargs...)\n\nLoads one or more packs into the main index from our pre-built artifacts.\n\nAvailability of packs might vary depending on your pipeline configuration (ie, whether we have the correct embeddings for it). See AIHelpMe.ALLOWED_PACKS\n\nExample\n\nload_index!(:julia)\n\nOr multiple packs\n\nload_index!([:julia, :makie,:tidier])\n\n\n\n\n\n","category":"method"},{"location":"reference/#AIHelpMe.load_index_hdf5-Tuple{AbstractString}","page":"Reference","title":"AIHelpMe.load_index_hdf5","text":"Hacky function to load a HDF5 file into a ChunkIndex object. Only bare-bone ChunkIndex is supported right now.\n\n\n\n\n\n","category":"method"},{"location":"reference/#AIHelpMe.update_index","page":"Reference","title":"AIHelpMe.update_index","text":"update_index(index::RT.AbstractChunkIndex = MAIN_INDEX[],\n    modules::Vector{Module} = Base.Docs.modules;\n    verbose::Integer = 1,\n    kwargs...)\n\nUpdates the provided index with the documentation of the provided modules.\n\nDeduplicates against the index.sources and embeds only the new document chunks (as measured by a hash).\n\nReturns the updated index (new instance).\n\nFor available configurations and customizations, see the corresponding modules and functions of PromptingTools.Experimental.RAGTools (eg, build_index).\n\nExample\n\nIf you loaded some new packages and want to add them to your MAIN_INDEX (or any index you use), run:\n\n# To update the MAIN_INDEX as well\nAIHelpMe.update_index() |> AHM.load_index!\n\n# To update an explicit index\nindex = AIHelpMe.update_index(index)\n\n\n\n\n\n","category":"function"},{"location":"reference/#AIHelpMe.update_pipeline!","page":"Reference","title":"AIHelpMe.update_pipeline!","text":"update_pipeline!(option::Symbol = :bronze; model_chat = MODEL_CHAT,\n    model_embedding = MODEL_EMBEDDING, verbose::Bool = true, truncate_dimension::Union{\n        Nothing, Integer} = nothing)\n\nUpdates the default RAG pipeline to one of the pre-configuration options and sets the requested chat and embedding models.\n\nThis is a good way to update model types to change between OpenAI models and Ollama models.\n\nSee available pipeline options via keys(RAG_CONFIGURATIONS).\n\nLogic:\n\nUpdates the global MODEL_CHAT and MODEL_EMBEDDING to the requested models.\nUpdates the global RAG_CONFIG and RAG_KWARGS to the requested option.\nUpdates the global LOADED_CONFIG_KEY to the configuration key for the given option and kwargs (used by the artifact system to download the correct knowledge packs).\n\nExample\n\nupdate_pipeline!(:bronze; model_chat = \"gpt4t\")\n\nYou don't need to re-load your index if you just change the chat model.\n\nYou can switch the pipeline to Ollama models: Note: only 1 Ollama model is supported for embeddings now! You must select \"nomic-embed-text\" and if you do, set truncate_dimension=0 (maximum dimension available)\n\nupdate_pipeline!(:bronze; model_chat = \"mistral:7b-instruct-v0.2-q4_K_M\",model_embedding=\"nomic-embed-text, truncate_dimension=0)\n\n# You must download the corresponding knowledge packs via `load_index!` (because you changed the embedding model)\nload_index!(:julia) # or whichever other packs you want!\n\n\n\n\n\n","category":"function"},{"location":"reference/#PromptingTools.Experimental.RAGTools.build_index","page":"Reference","title":"PromptingTools.Experimental.RAGTools.build_index","text":"RT.build_index(modules::Vector{Module} = Base.Docs.modules; verbose::Int = 1,\n    kwargs...)\n\nBuild index from the documentation of the currently loaded modules. If modules is empty, it will use all currently loaded modules.\n\n\n\n\n\n","category":"function"},{"location":"reference/#PromptingTools.Experimental.RAGTools.build_index-Tuple{Module}","page":"Reference","title":"PromptingTools.Experimental.RAGTools.build_index","text":"RT.build_index(mod::Module; verbose::Int = 1, kwargs...)\n\nBuild index from the documentation of a given module mod.\n\n\n\n\n\n","category":"method"},{"location":"reference/#AIHelpMe.@aihelp!_str-Tuple{Any, Vararg{Any}}","page":"Reference","title":"AIHelpMe.@aihelp!_str","text":"aihelp!\"user_question\"[model_alias] -> AIMessage\n\nThe aihelp!\"\" string macro is used to continue a previous conversation with the AI model. \n\nIt appends the new user prompt to the last conversation in the tracked history (in AIHelpMe.CONV_HISTORY) and generates a response based on the entire conversation context. If you want to see the previous conversation, you can access it via AIHelpMe.CONV_HISTORY, which keeps at most last PromptingTools.MAX_HISTORY_LENGTH conversations.\n\nIt does NOT provide new context from the documentation. To do that, start a new conversation with aihelp\"<question>\".\n\nArguments\n\nuser_question (String): The follow up question to be added to the existing conversation.\nmodel_alias (optional, any): Specify the model alias of the AI model to be used (see PT.MODEL_ALIASES). If not provided, the default model is used.\n\nReturns\n\nAIMessage corresponding to the new user prompt, considering the entire conversation history.\n\nExample\n\nTo continue a conversation:\n\n# start conversation as normal\naihelp\"How to create a dictionary?\" \n\n# ... wait for reply and then react to it:\n\n# continue the conversation (notice that you can change the model, eg, to more powerful one for better answer)\naihelp!\"Can you create it from named tuple?\"gpt4t\n# AIMessage(\"Yes, you can create a dictionary from a named tuple ...\")\n\nUsage Notes\n\nThis macro should be used when you want to maintain the context of an ongoing conversation (ie, the last ai\"\" message).\nIt automatically accesses and updates the global conversation history.\nIf no conversation history is found, it raises an assertion error, suggesting to initiate a new conversation using ai\"\" instead.\n\nImportant\n\nEnsure that the conversation history is not too long to maintain relevancy and coherence in the AI's responses. The history length is managed by MAX_HISTORY_LENGTH.\n\n\n\n\n\n","category":"macro"},{"location":"reference/#AIHelpMe.@aihelp_str-Tuple{Any, Vararg{Any}}","page":"Reference","title":"AIHelpMe.@aihelp_str","text":"aihelp\"user_question\"[model_alias] -> AIMessage\n\nThe aihelp\"\" string macro generates an AI response to a given user question by using aihelp under the hood. It will automatically try to provide the most relevant bits of the documentation (from the index) to the LLM to answer the question.\n\nSee also aihelp!\"\" if you want to reply to the provided message / continue the conversation.\n\nArguments\n\nuser_question (String): The question to be answered by the AI model.\nmodel_alias (optional, any): Provide model alias of the AI model (see MODEL_ALIASES).\n\nReturns\n\nAIMessage corresponding to the input prompt.\n\nExample\n\nresult = aihelp\"Hello, how are you?\"\n# AIMessage(\"Hello! I'm an AI assistant, so I don't have feelings, but I'm here to help you. How can I assist you today?\")\n\nIf you want to interpolate some variables or additional context, simply use string interpolation:\n\na=1\nresult = aihelp\"What is `$a+$a`?\"\n# AIMessage(\"The sum of `1+1` is `2`.\")\n\nIf you want to use a different model, eg, GPT-3.5 Turbo, you can provide its alias as a flag:\n\nresult = aihelp\"What is `1.23 * 100 + 1`?\"gpt3t\n# AIMessage(\"The answer is 124.\")\n\n\n\n\n\n","category":"macro"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = AIHelpMe","category":"page"},{"location":"#AIHelpMe:-\"AI-Enhanced-Coding-Assistance-for-Julia\"","page":"Home","title":"AIHelpMe: \"AI-Enhanced Coding Assistance for Julia\"","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for AIHelpMe.","category":"page"},{"location":"","page":"Home","title":"Home","text":"AIHelpMe harnesses the power of Julia's extensive documentation and advanced AI models to provide tailored coding guidance. By integrating with PromptingTools.jl, it offers a unique, AI-assisted approach to answering your coding queries directly in Julia's environment.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note: This is only a proof-of-concept. If there is enough interest, we will fine-tune the RAG pipeline for better performance.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"AI-Powered Assistance: Get context-aware answers to your coding questions.\nEasy-to-Use Interface: Simple function and macro to input your questions.\nFlexible Querying: Use different AI models for varied insights and performance versus cost trade-offs.\nCost-Effective: Download pre-embedded documentation to save on API calls.\nUniquely Tailored: Leverage the currently loaded documentation, regardless of whether it's private or public.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install AIHelpMe, use the Julia package manager and the address of the repository (it's not yet registered):","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(url=\"https://github.com/svilupp/AIHelpMe.jl\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Prerequisites:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Julia (version 1.10 or later).\nInternet connection for API access.\nOpenAI API keys with available credits. See How to Obtain API Keys.\nFor optimal performance, get also Cohere API key (free for community use) and Tavily API key (free for community use).","category":"page"},{"location":"","page":"Home","title":"Home","text":"All setup should take less than 5 minutes!","category":"page"},{"location":"#Quick-Start-Guide","page":"Home","title":"Quick Start Guide","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Basic Usage:\nusing AIHelpMe\naihelp(\"How do I implement quicksort in Julia?\")\n ```\n\n ```plaintext\n[ Info: Done generating response. Total cost: $0.015\nAIMessage(\"To implement quicksort in Julia, you can use the `sort` function with the `alg=QuickSort` argument.\")\nNote: As a default, we load only the Julia documentation and docstrings for standard libraries. The default model used is GPT-4 Turbo.\nYou can pretty-print the answer using pprint if you return the full RAGResult (return_all=true):\nusing AIHelpMe: pprint\n\nresult = aihelp(\"How do I implement quicksort in Julia?\", return_all=true)\npprint(result)\n--------------------\nQUESTION(s)\n--------------------\n- How do I implement quicksort in Julia?\n\n--------------------\nANSWER\n--------------------\nTo implement quicksort in Julia, you can use the [5,1.0]`sort`[1,1.0] function with the [1,1.0]`alg=QuickSort`[1,1.0] argument.[2,1.0]\n\n--------------------\nSOURCES\n--------------------\n1. https://docs.julialang.org/en/v1.10.2/base/sort/index.html::Sorting and Related Functions/Sorting Functions\n2. https://docs.julialang.org/en/v1.10.2/base/sort/index.html::Sorting and Related Functions/Sorting Functions\n3. https://docs.julialang.org/en/v1.10.2/base/sort/index.html::Sorting and Related Functions/Sorting Algorithms\n4. SortingAlgorithms::/README.md::0::SortingAlgorithms\n5. AIHelpMe::/README.md::0::AIHelpMe\nNote: You can see the model cheated because it can see this very documentation...\naihelp Macro:\naihelp\"how to implement quicksort in Julia?\"\nFollow-up Questions:\naihelp!\"Can you elaborate on the `sort` function?\"\nNote: The ! is required for follow-up questions. aihelp! does not add new context/more information - to do that, you need to ask a new question.\nPick faster models:  Eg, for simple questions, GPT 3.5 might be enough, so use the alias \"gpt3t\":  julia  aihelp\"Elaborate on the `sort` function and quicksort algorithm\"gpt3t\nplaintext  [ Info: Done generating response. Total cost: $0.002 -->  AIMessage(\"The `sort` function in programming languages, including Julia.... continues for a while!\nDebugging: How did you come up with that answer? Check the \"context\" provided to the AI model (ie, the documentation snippets that were used to generate the answer):  ```julia  AIHelpMe.pprint(AIHelpMe.LAST_RESULT[])\nOutput: Pretty-printed Question + Context + Answer with color highlights\n```\nThe color highlights show you which words were NOT supported by the provided context (magenta = completely new, blue = partially new).   It's a quite and intuitive way to see which function names or variables are made up versus which ones were in the context. \nYou can change the kwargs of pprint to hide the annotations or potentially even show the underlying context (snippets from the documentation):\njulia  AIHelpMe.pprint(AIHelpMe.LAST_RESULT[]; add_context = true, add_scores = false)","category":"page"},{"location":"#How-to-Obtain-API-Keys","page":"Home","title":"How to Obtain API Keys","text":"","category":"section"},{"location":"#OpenAI-API-Key:","page":"Home","title":"OpenAI API Key:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Visit OpenAI's API portal.\nSign up and generate an API Key.\nCharge some credits (5 minimum but that will last you for a lost time).\nSet it as an environment variable or a local preference in PromptingTools.jl. See the instructions.","category":"page"},{"location":"#Cohere-API-Key:","page":"Home","title":"Cohere API Key:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Sign up at Cohere's registration page.\nAfter registering, visit the API keys section to obtain a free, rate-limited Trial key.\nSet it as an environment variable or a local preference in PromptingTools.jl. See the instructions.","category":"page"},{"location":"#Tavily-API-Key:","page":"Home","title":"Tavily API Key:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Sign up at Tavily.\nAfter registering, generate an API key on the Overview page. You can get a free, rate-limited Trial key.\nSet it as an environment variable or a local preference in PromptingTools.jl. See the instructions.","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Formulating Questions:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Be clear and specific for the best results. Do mention the programming language (eg, Julia) and the topic (eg, \"quicksort\") when it's not obvious from the context.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Example Queries:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Simple question: aihelp\"What is a DataFrame in Julia?\"\nUsing a model: aihelp\"best practices for error handling in Julia\"gpt4t\nFollow-up: aihelp!\"Could you provide an example?\"\nDebug errors (use err REPL variable):","category":"page"},{"location":"","page":"Home","title":"Home","text":"## define mock function to trigger method error\nf(x::Int) = x^2\nf(Int8(2))\n# we get: ERROR: MethodError: no method matching f(::Int8)\n\n# Help is here:\naihelp\"What does this error mean? $err\" # Note the $err to interpolate the stacktrace","category":"page"},{"location":"","page":"Home","title":"Home","text":"[ Info: Done generating response. Total cost: $0.003\n\nAIMessage(\"The error message \"MethodError: no method matching f(::Int8)\" means that there is no method defined for function `f` that accepts an argument of type `Int8`. The error message also provides the closest candidate methods that were found, which are `f(::Any, !Matched::Any)` and `f(!Matched::Int64)` in the specified file `embed_all.jl` at lines 45 and 61, respectively.\")","category":"page"},{"location":"#Extending-the-Knowledge-Base","page":"Home","title":"Extending the Knowledge Base","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package by default ships with pre-processed embeddings for all Julia standard libraries, DataFrames and PromptingTools. Thanks to the amazing Julia Artifacts system, these embeddings are downloaded/cached/loaded every time the package starts.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note: The below functions are not yet exported. Prefix them with AIHelpMe. to use them.","category":"page"},{"location":"#Building-and-Updating-Indexes","page":"Home","title":"Building and Updating Indexes","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"AIHelpMe allows users to enhance its capabilities by embedding documentation from any loaded Julia module.  Utilize new_index = build_index(module) to create an index for a specific module (or a vector of modules). ","category":"page"},{"location":"","page":"Home","title":"Home","text":"To update an existing index, including newly imported packages, use new index = update_index(module) or simply update_index() to include all unrecognized modules. We will add and embed only the new documentation to avoid unnecessary duplication and cost.","category":"page"},{"location":"#Managing-Indexes","page":"Home","title":"Managing Indexes","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Once an index is built or updated, you can choose to serialize it for later use or set it as the primary index. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"To use your newly created index as the main source for queries, execute load_index!(new_index). Alternatively, load a pre-existing index from a file using load_index!(file_path). ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The main index for queries is held in the global variable AIHelpMe.MAIN_INDEX.","category":"page"},{"location":"#How-it-works","page":"Home","title":"How it works","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"AIHelpMe leverages PromptingTools.jl to communicate with the AI models.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We apply a Retrieval Augment Generation (RAG) pattern, ie, ","category":"page"},{"location":"","page":"Home","title":"Home","text":"we pre-process all available documentation (and \"embed it\" to convert text snippets into numbers)\nwhen a question is asked, we look up the most relevant documentation snippets\nwe feed the question and the documentation snippets to the AI model\nthe AI model generates the answer","category":"page"},{"location":"","page":"Home","title":"Home","text":"This ensures that the answers are not only based on general AI knowledge but are also specifically tailored to Julia's ecosystem and best practices.","category":"page"},{"location":"#Future-Directions","page":"Home","title":"Future Directions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"AIHelpMe is continuously evolving. Future updates may include:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Tools to trace the provenance of answers (ie, where did the answer come from?).\nCreation of a gold standard Q&A dataset for evaluation.\nRefinement of the RAG ingestion pipeline for optimized chunk processing and deduplication.\nIntroduction of context filtering to focus on specific modules.\nTransition to a more sophisticated multi-turn conversation design.\nEnhancement of the marginal information provided by the RAG context.\nExpansion of content sources beyond docstrings, potentially including documentation sites and community resources like Discourse or Slack posts.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Please note that this is merely a pre-release to gauge the interest in this project.","category":"page"},{"location":"faq/#Frequently-Asked-Questions","page":"F.A.Q","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"faq/","page":"F.A.Q","title":"F.A.Q","text":"Q: Is it expensive to embed all my documentation? A: No, embedding a comprehensive set of documentation is surprisingly cost-effective. Embedding around 170 modules, including all standard libraries and more, costs approximately 8 cents and takes less than 30 seconds.  To save you money, we have already embedded the Julia standard libraries and made them available for download via Artifacts.  We expect that any further knowledge base extensions should be at most a few cents (see Extending the Knowledge Base).","category":"page"},{"location":"faq/","page":"F.A.Q","title":"F.A.Q","text":"Q: How much does it cost to ask a question? A: Each query incurs only a fraction of a cent, depending on the length and chosen model.","category":"page"},{"location":"faq/","page":"F.A.Q","title":"F.A.Q","text":"Q: Can I use the Cohere Trial API Key for commercial projects? A: No, a trial key is only for testing purposes. But it takes only a few clicks to switch to Production API. The cost is only 1 per 1000 searches (!!!) and has many other benefits. Alternatively, set a different rerank_strategy in aihelp calls to avoid using Cohere API.","category":"page"},{"location":"faq/","page":"F.A.Q","title":"F.A.Q","text":"Q: How accurate are the answers? A: Like any other Generative AI answers, ie, it depends and you should always double-check.","category":"page"},{"location":"faq/","page":"F.A.Q","title":"F.A.Q","text":"Q: Can I use it without the internet? A: Not at the moment. It might be possible in the future, as PromptingTools.jl supports local LLMs.","category":"page"},{"location":"faq/","page":"F.A.Q","title":"F.A.Q","text":"Q: Why do we need Cohere API Key? A: Cohere's API is used to re-rank the best matching snippets from the documentation. It's free to use in limited quantities (ie, ~thousand requests per month), which should be enough for most users. Re-ranking improves the quality and accuracy of the answers.","category":"page"}]
}
